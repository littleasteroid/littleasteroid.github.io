### 人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。
### 机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。

### 机器学习算法大致可以分为三类：
1.监督学习算法 (Supervised Algorithms）:
- 在监督学习训练过程中，可以由训练数据集学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。
- 该算法要求特定的输入/输出，首先需要决定使用哪种数据作为范例。例如，文字识别应用中一个手写的字符，或一行手写文字。
- 主要算法包括神经网络、支持向量机、最近邻居法、朴素贝叶斯法、决策树等。
2.无监督学习算法 (Unsupervised Algorithms):
- 这类算法没有特定的目标输出，算法将数据集分为不同的组。
3.强化学习算法 (Reinforcement Algorithms):
- 强化学习普适性强，主要基于决策进行训练，算法根据输出结果（决策）的成功或错误来训练自己，通过大量经验训练优化后的算法将能够给出较好的预测。
- 类似有机体在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。
- 在运筹学和控制论的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。

### 基本的机器学习算法：
- 线性回归算法 Linear Regression
- 支持向量机算法 (Support Vector Machine,SVM)
- 最近邻居/k-近邻算法 (K-Nearest Neighbors,KNN)
- 逻辑回归算法 Logistic Regression
- 决策树算法 Decision Tree
- k-平均算法 K-Means
- 随机森林算法 Random Forest
- 朴素贝叶斯算法 Naive Bayes
- 降维算法 Dimensional Reduction
- 梯度增强算法 Gradient Boosting

### 《基于深度强化学习的自适应作业车间调度问题研究》
摘要：针对传统调度算法实时性较差而难以应对复杂多变的实际生产调度环境等问题，提出一个基于析取图分派的深度强化学习调度模型。
该模型综合深度卷积神经网络和强化学习实时性、灵活性的优势，直接依据输入的加工状态进行行为策略选取，更贴近实际订单反应式生产制造系统的调度决策过程。
通过把利用析取图进行调度求解的过程转化为多阶段决策问题，用深度卷积神经网络模型拟合状态动作值函数，创新性地把制造系统加工状态特征数据表达为多通道图像并输入模型，
采用考虑优先级经验回放的竞争双层深度Q网络（DDDQNPR）训练模型，把启发式算法或分配规则作为调度决策候选行为，结合强化学习在线评价-执行机制，从而为每次调度决策选取最优组合行为策略。
实验结果表明，在小规模问题上，所提出的方法可以求得最优解，在大规模问题上，该方法可以求得优于任意单一规则的调度结果，同时与遗传算法的调度性能相当；
为了证明算法的泛化性和鲁棒性，在训练代理时使用带有随机初始状态的案例作为验证集以选择泛化性能最优的模型，然后测试学习到的策略在具有不同初始状态的调度案例上的性能，
结果表明代理可以自适应地获得较优解，同时对工时不确定的动态案例进行了实验研究，结果表明，该方法在动态环境下仍然快速地获得鲁棒解。

- 析取图：
析取图就是disjunctive graph, 用来表示Job shop scheduling的过程。 
简单地说其中的方块和圆是用来表示一道工序，实心的箭头用来表示一个合同或工作(Job)所需要经过的流程(Operation)。
由于不同的合同所需要经过的流程有可能是同一个机器上完成的（即不同的合同需要同一个流程），所以为了表示这一关系，使用虚线将同一个机器上完成的工序连接起来。
通过析取图来表达调度问题的解，实际上就是在满足顺序约束和能力约束的基础上，确定各个工序的顺序，本质上为序列决策问题，当然就可以通过强化学习进行训练。

- 论文贡献：
（1）提出了使用考虑了优先级经验回放的dueling double DQN模型（DDDQNPR）来构建调度问题的深度强化学习框架，该框架中包含了目标网络和估计网络，以解决一般DQN存在的过高估计问题；
（2）首次建立了基于析取图模型的强化学习环境，将基于析取图的调度求解过程转化为序列决策过程。调度可以从非零的状态开始，即可以先交互式地安排一些工序，然后再对剩余工序进行优化调度；
（3）在每一离散时间步，将调度状态创新性地表示为多通道图像，避免了传统强化学习手动构造调度特征的步骤，CNN根据输入的状态进行启发式规则选择，从当前可调度任务集合中选择最优先的工件；
（4）设计了一种新颖的与制造期等效的奖励函数，用来评价每一次分派时对调度目标的影响；
（5）提出了一种改进的考虑精英策略的epsilon-decreasing策略，该策略在训练后期将以一定的概率选择当前最优解中的最优规则，实验结果表明，该策略在所有案例上的调度性能平均提升5.92%。
（6）进行了大量的实验研究，分析了不同超参数的灵敏度，验证了所提出方法在静态问题上的有效性，以及在反应式调度和工时不确定的动态问题上的泛化性。



- 将调度问题表达为强化学习问题并进行求解。
1.状态特征表达：
工时层、结果层、机床层
2.系统动作定义：
3.报酬函数设计：
4.探索和利用策略：


- 深度强化学习调度算法：


- 超参数灵敏度分析：


- 训练过程：



- 实验结果：







