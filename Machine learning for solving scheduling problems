### 人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。
### 机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。

### 机器学习算法大致可以分为三类：
1.监督学习算法 (Supervised Algorithms）:
- 在监督学习训练过程中，可以由训练数据集学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。
- 该算法要求特定的输入/输出，首先需要决定使用哪种数据作为范例。例如，文字识别应用中一个手写的字符，或一行手写文字。
- 主要算法包括神经网络、支持向量机、最近邻居法、朴素贝叶斯法、决策树等。
2.无监督学习算法 (Unsupervised Algorithms):
- 这类算法没有特定的目标输出，算法将数据集分为不同的组。
3.强化学习算法 (Reinforcement Algorithms):
- 强化学习普适性强，主要基于决策进行训练，算法根据输出结果（决策）的成功或错误来训练自己，通过大量经验训练优化后的算法将能够给出较好的预测。
- 类似有机体在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。
- 在运筹学和控制论的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。

### 基本的机器学习算法：
- 线性回归算法 Linear Regression
- 支持向量机算法 (Support Vector Machine,SVM)
- 最近邻居/k-近邻算法 (K-Nearest Neighbors,KNN)
- 逻辑回归算法 Logistic Regression
- 决策树算法 Decision Tree
- k-平均算法 K-Means
- 随机森林算法 Random Forest
- 朴素贝叶斯算法 Naive Bayes
- 降维算法 Dimensional Reduction
- 梯度增强算法 Gradient Boosting


### 强化学习
- 强化学习算法可以分为三大类：value based, policy based 和 actor critic。
- 常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络；
  以DDPG,TRPO为代表的actor-critic算法，这种算法中既有值函数网络，又有policy网络。
- 说到DQN中有值函数网络，这里简单介绍一下强化学习中的一个概念，叫值函数近似。
  一个state action pair (s,a)(s,a)对应一个值函数Q(s,a)Q(s,a)。理论上对于任意的(s,a)(s,a)我们都可以由公式求出它的值函数，即用一个查询表lookup table来表示值函数。
  但是当state或action的个数过多时，分别去求每一个值函数会很慢。因此我们用函数近似的方式去估计值函数：^Q(s,a,w)≈Qπ(s,a)Q^(s,a,w)≈Qπ(s,a)
  这样，对于未出现的state action也可以估计值函数。
  至于近似函数，DQN中用的是神经网络，当然如果环境比较简单的话用线性函数来近似也是可以的。
- 直观上来说，强化学习是智能体与环境不断交互，从而不断强化自己的决策能力的过程。
  首先环境(Env)会给智能体(Agent)一个观测值(Observation)(有时也称状态State)，智能体接收到环境给的观测值之后会做出一个动作(Action)，
  环境接收到智能体给的动作之后会做出一系列的反应，例如对这个动作给予一个奖励(Reward)，以及给出一个新的观测值。
  智能体根据环境给予的奖励值去更新自己的策略(Policy)。可以说，强化学习的目的就是为了得到最优的策略。
- 强化学习中的基本概念:
  1.策略Policy
  Policy是智能体的行为函数，是一个从状态到动作的映射，它告诉智能体如何挑选下一个action。
  强化学习中有两类policy: Deterministic policy和Stochastic policy。
  前一种又称确定性策略，即对于这个映射，输入一个状态，输出的是某一个确定的action。后一种是不确定性策略，即对于这个映射，输入一个状态，输出的是每个action的概率。
  2.Episode
  一个Episode由一系列的observation, reward, action组成:
  (O1,A1,R2,...,O(T−1),A(T−1),R(T),O(T))
  从initial observation到terminal observation。
  3.奖励Reward
  奖励Rt是一个反馈信号，是一个数值，表明这个智能体在step t 时做得有多好。
  4.Return
  又称累积折扣奖励(cumulative discounted reward)。step t 时的return为:
  Gt=R(t+1)+γR(t+2)+(γ^2)R(t+3)+...
  其中γ表示折扣因子，表示你对之后得到的reward的重视程度。0<=γ<=1，是一个由用户决定的参数。
  智能体的任务就是去最大化累积奖励。然而由上面的式子我们可以看出，一条样本（即一个episode）对应一个Return，而episode是不确定的，有很多种可能发生的情况，因此Return是一个变量。
  因此智能体的任务是最大化累积奖励的期望，即下面的值函数。
  5.值函数Value Function
  一个状态state s对应的值函数为:
  Vπ(s)=Eπ(Gt|St=s)
  这种值函数也称状态值函数。对给定的s, V(s)是一个确定的值。它表示，从state s开始，遵循策略π时的return的期望值。还有一种值函数称为状态动作值函数:
  Qπ(s,a)=Eπ(Gt|St=s,At=a)
  它表示，从state s开始， 采取动作a，遵循策略π时的return的期望值。
- 强化学习中的两类问题:
智能体的学习过程是一个反复与环境进行交互，不断试错又不断进步的过程。在这个迭代过程中的每一步，我们需要完成强化学习中的两个问题：
1、Prediction 即给定一个策略，求值函数
2、Control 根据值函数寻找最优策略
- 强化学习与传统机器学习的区别:
1、不同于传统机器学习的有监督或无监督，强化学习基于的是reward。
2、强化学习的数据是有序的，而传统机器学习的数据是iid(在传统机器学习中，数据分布在同一个机器上，且假设数据是从同一个分布中独立地采样的，即数据独立同分布(IID))的。
   因此传统机器学习的分布式系统对于强化学习不可用。
3、智能体的动作对于后续接收到的数据有影响。



### 强化学习-DQN算法



### 文献阅读：
《基于对偶双DQN的自适应作业车间调度问题研究》  ——IEEE Access,2020










