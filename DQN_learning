### 人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。
### 机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。

### 机器学习算法大致可以分为三类：
1.监督学习算法 (Supervised Algorithms）:
- 在监督学习训练过程中，可以由训练数据集学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。
- 该算法要求特定的输入/输出，首先需要决定使用哪种数据作为范例。例如，文字识别应用中一个手写的字符，或一行手写文字。
- 主要算法包括神经网络、支持向量机、最近邻居法、朴素贝叶斯法、决策树等。
2.无监督学习算法 (Unsupervised Algorithms):
- 这类算法没有特定的目标输出，算法将数据集分为不同的组。
3.强化学习算法 (Reinforcement Algorithms):
- 强化学习普适性强，主要基于决策进行训练，算法根据输出结果（决策）的成功或错误来训练自己，通过大量经验训练优化后的算法将能够给出较好的预测。
- 类似有机体在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。
- 在运筹学和控制论的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。

### 基本的机器学习算法：
- 线性回归算法 Linear Regression
- 支持向量机算法 (Support Vector Machine,SVM)
- 最近邻居/k-近邻算法 (K-Nearest Neighbors,KNN)
- 逻辑回归算法 Logistic Regression
- 决策树算法 Decision Tree
- k-平均算法 K-Means
- 随机森林算法 Random Forest
- 朴素贝叶斯算法 Naive Bayes
- 降维算法 Dimensional Reduction
- 梯度增强算法 Gradient Boosting

### 深度强化学习（DRL）简介与常见算法（DQN，DDPG，PPO，TRPO，SAC）分类（https://blog.csdn.net/b_b1949/article/details/128997146）
- 强化学习属于机器学习的一种，不同于监督学习和无监督学习，通过智能体与环境的不断交互(即采取动作)进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。
  强化学习特别适合序贯决策问题（涉及一系列有序的决策问题）。
  在实际应用中，针对某些任务，我们往往无法给每个数据或者状态贴上准确的标签，但是能够知道或评估当前情况或数据是好还是坏，可以采用强化学习来处理。
1.强化学习：
  1.1定义：智能体与环境的不断交互(即在给定状态采取动作)进而获得奖励，此时环境从一个状态转移到下一个状态。智能体通过不断优化自身动作策略，以期待最大化其长期回报或收益(奖励之和)。
  1.2强化学习的相关概念
  （1）状态 State (S): agent’s observation of its environment;
  （2）动作 Action (A): the approaches that agent interacts with the environment;
  （3）奖励 Reward (Rt): the bonus that agent get once it takes an action in the environment at the given time step t .回报（Return）为Agent所获得的奖励之和。
  （4）转移概率 Transistion Probability ( P ): the transition possibility that environment evolves from one state to another.
       环境从一个状态转移到另一个状态，可以是确定性转移过程，也可以是随机性转移过程.
  （5）折扣因子 Discount factor ( γ \gammaγ ): to measure the importance of future reward to agent at the current state.
  （6）轨迹（Trajectory）是一系列的状态、动作、和奖励，可以表述为：τ = (S0, A0, R0, S1, A1, R1, . . . ) 
       用轨迹τ来记录Agent如何和环境交互。轨迹的初始状态是从起始状态分布中随机采样得到的。
       一条轨迹有时候也称为片段（Episode）或者回合，是一个从初始状态（Initial State，例如游戏的开局）到最终状态（Terminal State，如游戏中死亡或者胜利）的序列。
  （7）探索-利用的折中（Exploration-Exploitation Tradeoff）
       这里，探索是指Agent通过与环境的交互来获取更多的信息，而利用是指使用当前已知信息来使得Agent的表现达到最佳，例如，贪心（greedy）策略。
       同一时间，只能二者选一。因此，如何平衡探索和利用二者，以实现长期回报（Long-term Return）最大，是强化学习中非常重要的问题。
     因此，可以用（S，A，P，R，γ）来描述强化学习过程。
  1.3强化学习的数学建模
2.深度强化学习：
  深度学习DL有很强的抽象和表示能力，特别适合建模RL中的值函数，例如：动作价值函数Qπ(s,a)。二者结合，极大地拓展了RL的应用范围。
3.常见深度强化学习算法：
  深度强化学习的算法比较多，常见的有：DQN，DDPG，PPO，TRPO，A3C，SAC 等等。
  3.1Deep Q-Networks(DQN)
    DQN网路将Q-Learning和深度学习结合起来，并引入了两种新颖的技术来解决以往采用神经网络等非线性函数逼近器表示动作价值函数Q(s,a)所产生的不稳定性问题：
    技术1：经验回放缓存（Replay Buffer）：将Agent获得的经验存入缓存中，然后从该缓存中均匀采用（也可考虑基于优先级采样）小批量样本用于Q-Learning的更新；
    技术2：目标网络（Target Network）：引入独立的网络，用来代替所需的Q网络来生成Q-Learning的目标，进一步提高神经网络稳定性。
    这里，技术1能够提高样本使用效率，降低样本间相关性，平滑学习过程；技术2能够使目标值不受最新参数的影响，大大较少发散和震荡。
    通过在Atari 中多种游戏上进行测试，该DQN算法表现出了优异的性能。
    DQN算法具体描述如下：
    https://blog.csdn.net/b_b1949/article/details/128997146
4.深度强化学习算法分类
  深度强化学习算法的分类标准很多，常见的分类方式和具体类别概述如下：
  4.1根据Agent训练与测试所采用的策略是否一致
    4.1.1 off-policy （离轨策略、离线策略）
      Agent在训练（产生数据）时所使用的策略π1与 agent测试（方法评估与提升）时所用的策略π2不一致。
      例如，在DQN算法中，训练时，通常采用ϵ−greedy策略；而在测试性能或者实际使用时，采用a∗=arg maxQπ(s,a)策略。
      常见算法有：DDPG，TD3，Q-learning，DQN等。
    4.1.2 on-policy （同轨策略、在线策略）
      Agent在训练时（产生数据）所使用的策略与其测试（方法评估与提升）时使用的策略为同一个策略π \piπ。
      常见算法有：Sarsa，Policy Gradient，TRPO，PPO，A3C等。
   4.2 策略优化的方式不同
     4.2.1 Value-based algorithms（基于价值的算法）
      基于价值的方法通常意味着对动作价值函数Qπ(s,a)的优化，最优策略通过选取该函数Qπ(s,a)最大值所对应的动作，即π∗≈arg ⁡max ⁡Qπ(s,a)，这里，≈由函数近似误差导致。
      优点:基于价值的算法具有采样效率相对较高，值函数估计方差小，不易陷入局部最优.
      缺点:通常不能处理连续动作空间问题，最终策略通常为确定性策略。
      常见算法有 Q-learning，DQN，Double DQN，等，适用于 Discrete action space。其中，DQN算法是基于state-action function Q ( s , a ) Q(s,a)Q(s,a) 来进行选择最优action的。
    4.2.2 Policy-based algorithms（基于策略的算法）
      基于策略的方法直接对策略进行优化，通关对策略迭代更新，实现累计奖励（回报）最大化。其具有策略参数化简单、收敛速度快的优点，而且适用于连续或者高维动作空间。
      策略梯度方法（Policy Gradient Method，PGM）是一类直接针对期望回报通过梯度下降（Gradient Descent，针对最小化问题）进行策略优化的强化学习方法。
      其不需要在动作空间中求解价值最大化的优化问题，从而比较适用于 continuous and high-Dimension action space，也可以自然地对随机策略进行建模。
      PGM方法通过梯度上升的方法直接在神经网络的参数上优化Agent的策略。
    4.2.3 Actor-Critic algorithms （演员-评论家方法）
      Actor-Critic方法结合了上述基于价值的方法和基于策略的方法，利用基于价值的方法学习Q值函数或状态价值函数V来提高采样效率（Critic），
      并利用基于策略的方法学习策略函数（Actor），从而适用于连续或高维动作空间。其缺点也继承了二者的缺点，例如，Critic存在过估计问题，而Actor存在探索不足的问题等。
      常见算法有 DDPG, A3C，TD3，SAC，等，适用于 continuous and high-Dimension action space.
  4.3 参数更新的方式不同:Parameters updating methods
    4.3.1 Monte Carlo method（蒙特卡罗方法）
      蒙特卡罗方法：必须等待一条轨迹τk生成（真实值）后才能更新。
      常见算法有：Policy Gradient，TRPO，PPO等。
    4.3.2 Temporal Difference method（时间差分方法）
      时间差分方法：在每一步动作执行都可以通过自举法（Bootstrapping）（估计值）及时更新。
      常见算法有：DDPG，Q-learning，DQN等。


### 强化学习
- 强化学习算法可以分为三大类：value based, policy based 和 actor critic。
- 常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络；
  以DDPG,TRPO为代表的actor-critic算法，这种算法中既有值函数网络，又有policy网络。
- 说到DQN中有值函数网络，这里简单介绍一下强化学习中的一个概念，叫值函数近似。
  一个state action pair (s,a)(s,a)对应一个值函数Q(s,a)Q(s,a)。理论上对于任意的(s,a)(s,a)我们都可以由公式求出它的值函数，即用一个查询表lookup table来表示值函数。
  但是当state或action的个数过多时，分别去求每一个值函数会很慢。因此我们用函数近似的方式去估计值函数：^Q(s,a,w)≈Qπ(s,a)Q^(s,a,w)≈Qπ(s,a)
  这样，对于未出现的state action也可以估计值函数。
  至于近似函数，DQN中用的是神经网络，当然如果环境比较简单的话用线性函数来近似也是可以的。
- 直观上来说，强化学习是智能体与环境不断交互，从而不断强化自己的决策能力的过程。
  首先环境(Env)会给智能体(Agent)一个观测值(Observation)(有时也称状态State)，智能体接收到环境给的观测值之后会做出一个动作(Action)，
  环境接收到智能体给的动作之后会做出一系列的反应，例如对这个动作给予一个奖励(Reward)，以及给出一个新的观测值。
  智能体根据环境给予的奖励值去更新自己的策略(Policy)。可以说，强化学习的目的就是为了得到最优的策略。
- 强化学习中的基本概念:
  1.策略Policy
  Policy是智能体的行为函数，是一个从状态到动作的映射，它告诉智能体如何挑选下一个action。
  强化学习中有两类policy: Deterministic policy和Stochastic policy。
  前一种又称确定性策略，即对于这个映射，输入一个状态，输出的是某一个确定的action。后一种是不确定性策略，即对于这个映射，输入一个状态，输出的是每个action的概率。
  2.Episode
  一个Episode由一系列的observation, reward, action组成:
  (O1,A1,R2,...,O(T−1),A(T−1),R(T),O(T))
  从initial observation到terminal observation。
  3.奖励Reward
  奖励Rt是一个反馈信号，是一个数值，表明这个智能体在step t 时做得有多好。
  4.Return
  又称累积折扣奖励(cumulative discounted reward)。step t 时的return为:
  Gt=R(t+1)+γR(t+2)+(γ^2)R(t+3)+...
  其中γ表示折扣因子，表示你对之后得到的reward的重视程度。0<=γ<=1，是一个由用户决定的参数。
  智能体的任务就是去最大化累积奖励。然而由上面的式子我们可以看出，一条样本（即一个episode）对应一个Return，而episode是不确定的，有很多种可能发生的情况，因此Return是一个变量。
  因此智能体的任务是最大化累积奖励的期望，即下面的值函数。
  5.值函数Value Function
  一个状态state s对应的值函数为:
  Vπ(s)=Eπ(Gt|St=s)
  这种值函数也称状态值函数。对给定的s, V(s)是一个确定的值。它表示，从state s开始，遵循策略π时的return的期望值。还有一种值函数称为状态动作值函数:
  Qπ(s,a)=Eπ(Gt|St=s,At=a)
  它表示，从state s开始， 采取动作a，遵循策略π时的return的期望值。
- 强化学习中的两类问题:
智能体的学习过程是一个反复与环境进行交互，不断试错又不断进步的过程。在这个迭代过程中的每一步，我们需要完成强化学习中的两个问题：
1、Prediction 即给定一个策略，求值函数
2、Control 根据值函数寻找最优策略
- 强化学习与传统机器学习的区别:
1、不同于传统机器学习的有监督或无监督，强化学习基于的是reward。
2、强化学习的数据是有序的，而传统机器学习的数据是iid(在传统机器学习中，数据分布在同一个机器上，且假设数据是从同一个分布中独立地采样的，即数据独立同分布(IID))的。
   因此传统机器学习的分布式系统对于强化学习不可用。
3、智能体的动作对于后续接收到的数据有影响。
# 强化学习类型：
- 知道环境的模型。这在强化学习中也叫Model Based RL。即我们知道状态转移概率矩阵：当处于(s,a)时，下一个到达的状态可能是什么，并且到达每一个状态的概率是什么；
  我们还知道奖励函数：当处于(s,a)时，得到的立即回报的期望值是什么；另外还知道折扣因子。由此，我们便可以通过贝尔曼方程来求解值函数。
  这种情况下我们可能也并不需要神经网络近似值函数之类的，直接由策略迭代或值迭代便可以求出最优策略。具体方法可以看一下MDP。
- Model Free RL：不管有没有环境模型，反正我不用。那么在不知道环境模型的情况下如何求解值函数呢？答案就是采样。强化学习中有多种采样的方法。

### 强化学习-DQN算法
- DQN属于Model Free的强化学习算法，它需要采样。且同SarSa类似，只依赖下一个step的值函数。但它更新值函数的方式与SarSa又有所不同。
- DQN是使用ϵ−greedy策略来输出action，SarSa和Q-Learning也是。
  SarSa中使用ϵ−greedy策略生成action at+1，随即又用 at+1处对应的值函数来计算target，更新上一步的值函数。这种学习方式又称为On-policy。
  SarSa属于On-policy Learning，而Q-Learning属于Off-policy Learning。
- Off-policy是Q-Learning的特点，DQN中也延用了这一特点。而不同的是，Q-Learning中用来计算target和预测值的Q是同一个Q，也就是说使用了相同的神经网络。
  这样带来的一个问题就是，每次更新神经网络的时候，target也都会更新，这样会容易导致参数不收敛。而在有监督学习中，标签label都是固定的，不会随着参数的更新而改变。
  因此DQN在原来的Q网络的基础上又引入了一个target Q网络，即用来计算target的网络。
  它和Q网络结构一样，初始的权重也一样，只是Q网络每次迭代都会更新，而target Q网络是每隔一段时间才会更新。
  DQN的target是 Rt+1+γmaxa′Q(St+1,a′;ω−)。用 ω−表示它比Q网络的权重 ω更新得要慢一些。
- DQN所做的改进：
  相比于Q-Learning，DQN做的改进：一个是使用了卷积神经网络来逼近行为值函数，一个是使用了target Q network来更新target，还有一个是使用了经验回放Experience replay。
  由于在强化学习中，我们得到的观测数据是有序的，step by step的，用这样的数据去更新神经网络的参数会有问题。回忆在有监督学习中，数据之间都是独立的。
  因此DQN中使用经验回放，即用一个Memory来存储经历过的数据，每次更新参数的时候从Memory中抽取一部分的数据来用于更新，以此来打破数据间的关联。
- DQN算法流程：
  1.首先初始化Memory D，它的容量为N;
  2.初始化Q网络，随机生成权重ω;
  3.初始化target Q网络，权重为ω−=ω;
  4.循环遍历episode =1, 2, …, M:
  5.初始化initial state S1;
  6.循环遍历step =1,2,…, T:
    6.1.用ϵ−greedy策略生成action at：以ϵ概率选择一个随机的action，或选择at=maxaQ(St,a;ω);
    6.2.执行action at，接收reward rt及新的state St+1;
    6.3.将transition样本 (St,at,rt,St+1)存入D中；
    6.4.从D中随机抽取一个minibatch的transitions (Sj,aj,rj,Sj+1)；
    6.5.如果 j+1步是terminal的话，令yj=rj，否则，令 yj=rj+γmaxa′Q(St+1,a′;ω−)；
    6.6.对(yj−Q(St,aj;ω))2关于ω使用梯度下降法进行更新；
    6.7.每隔C steps更新target Q网络，ω−=ω。
  End For;
  End For.
- ϵ−greedy策略：
  greedy策略，顾名思义，是一种贪婪策略，它每次都选择使得值函数最大的action，即at=maxaQ(St,a;ω)。
  但是这种方式有问题，就是对于采样中没有出现过的(state, action) 对，由于没有评估，没有Q值，之后也不会再被采到。
  其实这里涉及到了强化学习中一个非常重要的概念，叫Exploration & Exploitation，探索与利用。
  前者强调发掘环境中的更多信息，并不局限在已知的信息中；后者强调从已知的信息中最大化奖励。而greedy策略只注重了后者，没有涉及前者。所以它并不是一个好的策略。
  而ϵ−greedy策略兼具了探索与利用，它以ϵ的概率从所有的action中随机抽取一个，以1−ϵ的概率抽取at=maxaQ(St,a;ω)。
  强化学习正是因为有了探索Exploration，才会常常有一些出人意表的现象，才会更加与其他机器学习不同。例如智能体在围棋游戏中走出一些人类经验之外的好棋局。







### 文献阅读：
《基于对偶双DQN的自适应作业车间调度问题研究》  ——IEEE Access,2020










